{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac097741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import allel\n",
    "from numba import njit\n",
    "import malariagen_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "import zarr\n",
    "\n",
    "import locusPocus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a892ca",
   "metadata": {},
   "source": [
    "### Writing out haplotypes for phylogenetic analysis\n",
    "\n",
    "First, lets set the genome position of our focus, and also set a position for a separate analysis of a downstream and upstream region. As the coeae1f signal is so large, I set this to be 2 megabases away. We can also set the size of the flanking region in bp, I set this to 10kb either side of the focus (same as Xavi rdl ms).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f346144b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "region_names = ['focal', 'upstream', 'downstream', 'collinear']\n",
    "regions = [28_545_767, 26_545_767, 30_545_767, 45_000_000]\n",
    "\n",
    "locus_name = 'coeae1f'\n",
    "contig = '2L'\n",
    "flanking = 10_000\n",
    "\n",
    "out_prefix = \"results/phylo\" \n",
    "\n",
    "outgroup_h5_path = \"/home/sanj/ag1000g/data/outgroups/outgroup_alleles.h5\"\n",
    "phase1_positions_array_path = f\"/home/sanj/ag1000g/data/ag1000g.phase1.ar3.pass/{contig}/variants/POS/\"\n",
    "remove_2la_hets = True ### only if analysing inside 2la region\n",
    "\n",
    "sample_sets = [\n",
    "    # Ag1000G phase 3 sample sets in Ag3.0\n",
    "    \"AG1000G-GH\", \n",
    "    'AG1000G-ML-A',\n",
    "     'AG1000G-BF-A',\n",
    "     'AG1000G-BF-B',\n",
    "     'AG1000G-GN-A',\n",
    "     'AG1000G-GN-B',\n",
    "    'AG1000G-TZ',\n",
    "    # Amenta-Etego sample sets in Ag3.3\n",
    "    # GAARDIAN sample set in Ag3.4\n",
    "    '1244-VO-GH-YAWSON-VMF00149',\n",
    "    # GAARD Ghana sample set in Ag3.2\n",
    "     \"1244-VO-GH-YAWSON-VMF00051\",\n",
    "     '1245-VO-CI-CONSTANT-VMF00054',\n",
    "     '1253-VO-TG-DJOGBENOU-VMF00052',\n",
    "     '1237-VO-BJ-DJOGBENOU-VMF00050'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de710fa4",
   "metadata": {},
   "source": [
    "Then lets select which cohorts we want to load, connect to Ag3() API and load some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029c9424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(null);\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define region strings\n",
    "region_dict = dict(zip(region_names, regions))\n",
    "\n",
    "for k, v in region_dict.items():\n",
    "    region_dict[k] = f'{contig}:{v-flanking}-{v+flanking}'\n",
    "\n",
    "ag3 = malariagen_data.Ag3(pre=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "909f8e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b47b76c47844fabaed4ccbd08fca1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load sample metadata:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "gambiae                          1146\n",
       "coluzzii                         1041\n",
       "arabiensis                        228\n",
       "intermediate_gambiae_coluzzii      16\n",
       "Name: aim_species, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = ag3.sample_metadata(sample_sets)\n",
    "metadata['aim_species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66a9a0",
   "metadata": {},
   "source": [
    "### Write haplotypes to FASTA\n",
    "\n",
    "In this notebook, we will construct FASTA sequences (or a multi-sequence alignment) from haplotypes in selected cohorts of the ag3, plus some outgroups. We will use *An. merus*, *An. melas*, and *An. quadriannulatus* as outgroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715cfade",
   "metadata": {},
   "outputs": [],
   "source": [
    "outgroup_alleles = h5py.File(outgroup_h5_path, 'r')\n",
    "outgroup_names = ['meru', 'mela', 'quad'] # we will use these only \n",
    "\n",
    "phase1_pos = zarr.open_array(phase1_positions_array_path, mode='r')\n",
    "phase1_pos = allel.SortedIndex(phase1_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d34413",
   "metadata": {},
   "source": [
    "If we are analysing a region within 2La, we will also restrict analyses to individuals homozygous for either 2la or 2l+a, to aid interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6836f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1683 2la/2l+a homozygote individuals\n"
     ]
    }
   ],
   "source": [
    "if remove_2la_hets:\n",
    "    path_to_compkaryo_table = \"/home/sanj/projects/gaardian/results/gaard_and_ag3.2la.karyo.tsv\" # must be for all samples and cohorts in this analysis\n",
    "    \n",
    "    karyos = pd.read_csv(path_to_compkaryo_table, sep=\"\\t\", index_col=0)\n",
    "    hom_2la_partner_sample_ids = karyos.query(\"mean_genotype < 0.1 | mean_genotype > 1.8\")['partner_sample_id']\n",
    "    print(f\"There are {len(hom_2la_partner_sample_ids)} 2la/2l+a homozygote individuals\")\n",
    "\n",
    "    karyo_df = karyos.query(\"mean_genotype < 0.1 | mean_genotype > 1.8\")[['partner_sample_id', 'mean_genotype']]\n",
    "    karyo_df = karyo_df.rename(columns={'partner_sample_id':'sample_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bab57b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def haps_to_fasta_numba(haplos, var_alleles):\n",
    "    \"\"\"\n",
    "    Loop through haplotypes and ref/alt alleles to construct a haplotype FASTA. Numba accelerated.\n",
    "    \"\"\"\n",
    "    seq_list = [] \n",
    "    for i in range(haplos.shape[1]):\n",
    "        seq_arr = []\n",
    "\n",
    "        for gn, allele in enumerate(haplos[:,i]):\n",
    "            seq_arr.append(var_alleles[gn][allele])\n",
    "        \n",
    "        seq_list.append(np.array(seq_arr))\n",
    "    return(seq_list)\n",
    "\n",
    "def haps_to_fasta_df(haps):\n",
    "    \"\"\"\n",
    "    Take haplotype XArray and create pandas df of fasta sequences\n",
    "    \"\"\"\n",
    "    # transform xarray into haplotype array\n",
    "    haplos = allel.GenotypeArray(haps['call_genotype'].compute()).to_haplotypes()\n",
    "    # extract ref and alt alleles array\n",
    "    var_alleles = haps['variant_allele'].compute()\n",
    "    \n",
    "    seq_arr = haps_to_fasta_numba(haplos.values, var_alleles.values)\n",
    "    seq_arr = np.vstack(seq_arr)\n",
    "    \n",
    "    # Make dataframe of all haplotype sequences for region\n",
    "    fasta_df = pd.DataFrame(seq_arr)\n",
    "    fasta_df.columns = haps['variant_position'].compute().values\n",
    "    sample_ids = haps['sample_id'].compute().values\n",
    "    fasta_df.loc[:, 'hap'] = [\"> \" + p for p in locusPocus.rename_duplicates(np.repeat(sample_ids, 2))]\n",
    "    cols = list(fasta_df)\n",
    "    # move the column to head of list using index, pop and insert\n",
    "    cols.insert(0, cols.pop(cols.index('hap')))\n",
    "    fasta_df = fasta_df.loc[:, cols]\n",
    "    return(fasta_df)\n",
    "\n",
    "def remove_missing_invariant_fasta_df(fasta_df, remove_invariant=False):\n",
    "    \"\"\"\n",
    "    Remove any columns in pandas dataframe that are missing genotypes ('.')\n",
    "    \"\"\"\n",
    "    missing_bool = fasta_df.apply(lambda x: any(x == b'.') , axis=0)\n",
    "    print(f\"Removing {missing_bool.sum()} alleles with a missing call\")\n",
    "    fasta_df = fasta_df.loc[:, ~missing_bool]\n",
    "    fasta_df = fasta_df.set_index('hap')\n",
    "    \n",
    "    if remove_invariant:\n",
    "        invariant_cols = fasta_df.nunique() <= 1\n",
    "        print(f\"Removing {invariant_cols.sum()} invariant SNPs\")\n",
    "        fasta_df = fasta_df.loc[:, ~invariant_cols]\n",
    "        print(f\"There are {fasta_df.shape[0]} haplotypes and {fasta_df.shape[1]} segregating haplotype calls\")\n",
    "\n",
    "    fasta_df.loc[:, 'seq'] = fasta_df.apply(lambda x: b''.join(x).decode('utf8'), axis=1)\n",
    "    return(fasta_df.reset_index()[['hap', 'seq']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3999620",
   "metadata": {},
   "source": [
    "Optionally, add sweep ID columns to metadata to allow for visualise in R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a5c2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_ids = True\n",
    "\n",
    "if sweep_ids:\n",
    "    meta_sweeps = pd.read_csv(\"../../results/haplotype_metahaps.tsv\", sep=\"\\t\")\n",
    "    meta_sweeps = meta_sweeps[['sample_id', 'hap_cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45182b03",
   "metadata": {},
   "source": [
    "For each region, convert the haps to fasta and write to file! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a037650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_prefix = \"../../results/phylo\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2286b064",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading haplotypes | focal | 2L:28535767-28555767\n",
      "focal 2L:28535767-28555767 (6819, 2431, 2)\n",
      "Converting haps to FASTA sequence | focal\n",
      "Removing 201 alleles with a missing call\n",
      "Multiple alignment FASTA written \n",
      "\n",
      "loading haplotypes | upstream | 2L:26535767-26555767\n",
      "upstream 2L:26535767-26555767 (4359, 2431, 2)\n",
      "Converting haps to FASTA sequence | upstream\n",
      "Removing 85 alleles with a missing call\n",
      "Multiple alignment FASTA written \n",
      "\n",
      "loading haplotypes | downstream | 2L:30535767-30555767\n",
      "downstream 2L:30535767-30555767 (5131, 2431, 2)\n",
      "Converting haps to FASTA sequence | downstream\n",
      "Removing 64 alleles with a missing call\n",
      "Multiple alignment FASTA written \n",
      "\n",
      "loading haplotypes | collinear | 2L:44990000-45010000\n",
      "collinear 2L:44990000-45010000 (5061, 2431, 2)\n",
      "Converting haps to FASTA sequence | collinear\n",
      "Removing 84 alleles with a missing call\n",
      "Multiple alignment FASTA written \n",
      "\n"
     ]
    }
   ],
   "source": [
    "haps = {}\n",
    "for name, region in region_dict.items():\n",
    "    # Load haplotypes for region, find intersection with phase 1 outgroup data\n",
    "    print(f\"loading haplotypes | {name} | {region}\")\n",
    "    haps[name] = ag3.haplotypes(region=region, sample_sets=sample_sets, analysis='gamb_colu_arab')\n",
    "    print(name, region, haps[name]['call_genotype'].shape)\n",
    "    sample_ids = haps[name]['sample_id'].compute().values\n",
    "    outgroup_region_bool, phase1_bool = phase1_pos.locate_intersection(haps[name]['variant_position'].compute().values)\n",
    "    haps[name] = haps[name].sel(variants=phase1_bool)\n",
    "    \n",
    "    if remove_2la_hets:\n",
    "    ### for 2La ###\n",
    "        hom_2la_bool = np.isin(sample_ids, hom_2la_partner_sample_ids)\n",
    "        haps[name] = haps[name].sel(samples=hom_2la_bool)\n",
    "    \n",
    "    # for each haplotype, loop through SNPs and create a FASTA sequence array depending on alleles\n",
    "    print(f\"Converting haps to FASTA sequence | {name}\")\n",
    "    fasta_df = haps_to_fasta_df(haps[name])\n",
    "\n",
    "    # concat outgroup sequences\n",
    "    outgroup_contig = {}\n",
    "    for out in outgroup_names:\n",
    "        outgroup_contig[out] = outgroup_alleles[contig][out][:][outgroup_region_bool]\n",
    "        df = pd.DataFrame(outgroup_contig[out]).T\n",
    "        df.columns = haps[name]['variant_position'].compute().values\n",
    "        df.loc[:, 'hap'] = \"> \" + out\n",
    "        fasta_df = pd.concat([fasta_df, df], axis=0)\n",
    "    \n",
    "    # remove missing alleles in the outgroups and invariant sites \n",
    "    fasta_df = remove_missing_invariant_fasta_df(fasta_df, remove_invariant=False)\n",
    "    \n",
    "    # write to csv with \\n sep to make FASTA file\n",
    "    fasta_df.to_csv(f\"{out_prefix}/{locus_name}_{name}.fasta\", sep=\"\\n\", index=False, header=False)\n",
    "    print(f'Multiple alignment FASTA written \\n')\n",
    "    \n",
    "    # Make an artificial metadata table for the outgroups and concat to the bottom of normal metahaps\n",
    "    outgroup_n = len(outgroup_names)\n",
    "    outgroup_meta = metadata.iloc[:outgroup_n, :].copy()\n",
    "    outgroup_meta.iloc[:outgroup_n, :] = \"Nan\"\n",
    "    for i, out in enumerate(outgroup_names):\n",
    "        outgroup_meta.loc[i, 'sample_id'] = out\n",
    "        outgroup_meta.loc[i, 'aim_species'] = out\n",
    "        outgroup_meta.loc[i, 'cluster_id'] = out\n",
    "        \n",
    "    metahaps, q = locusPocus.load_metahaps(sample_sets, sample_ids)\n",
    "    if sweep_ids: metahaps.loc[:, 'hap_cluster'] = meta_sweeps.loc[:, 'hap_cluster']\n",
    "    if remove_2la_hets:\n",
    "        metahaps = metahaps.query(\"sample_id in @hom_2la_partner_sample_ids\")\n",
    "    meta = pd.concat([metahaps, outgroup_meta], axis=0)\n",
    "    \n",
    "    if remove_2la_hets:\n",
    "        meta = meta.merge(karyo_df, how='left')\n",
    "        meta.loc[:, 'karyotype'] = np.where(\n",
    "             meta['mean_genotype'].between(0, 0.4, inclusive=\"both\"), \n",
    "            '2l+a', \n",
    "             np.where(\n",
    "                meta['mean_genotype'].between(1.6, 2, inclusive=\"both\"), '2la', 'Unknown'\n",
    "             )\n",
    "        )\n",
    "    \n",
    "    # remove > and join with metadata for each pop, useful for plotting phylo trees metadata\n",
    "    fasta_df.loc[:, 'hap'] = fasta_df['hap'].str.strip('> ')\n",
    "    all_haps = pd.concat([fasta_df.reset_index(drop=True), meta.reset_index(drop=True)], axis=1)\n",
    "    all_haps.to_csv(f\"{out_prefix}/{locus_name}_{name}.metadata.tsv\", sep=\"\\t\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d20168",
   "metadata": {},
   "source": [
    "## Running IQTree\n",
    "You may prefer to do this outside of the jupyter notebook, it can take a while :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc1124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for name in names:\n",
    "#   !iqtree -s {locus_name}_{name}.fasta -B 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767a6d4",
   "metadata": {},
   "source": [
    "### Testing the speed of numba  vs naive python implementation\n",
    "\n",
    "A function which generated a FASTA sequence from the haplotype alleles was a bottleneck in computational time, so I made a numba implementation, and this turned out to be suuuuuper quick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df438458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haps_to_fasta_naive(haplos, var_alleles):\n",
    "    seq_list = [] \n",
    "    for i in range(haplos.shape[1]):\n",
    "        seq_arr = []\n",
    "\n",
    "        for gn, allele in enumerate(haplos[:,i]):\n",
    "            seq_arr.append(var_alleles[gn][allele])\n",
    "        \n",
    "        seq_list.append(np.array(seq_arr))\n",
    "    return(np.vstack(seq_list))\n",
    "\n",
    "\n",
    "def haps_to_fasta_eric(haplos, var_alleles):\n",
    "    return(var_alleles[range(var_alleles.shape[0]), np.transpose(haplos)])\n",
    "\n",
    "@njit\n",
    "def haps_to_fasta_eric_numba(haplos, var_alleles):\n",
    "    return(var_alleles[np.arange(var_alleles.shape[0]), np.transpose(haplos)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03bb4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#haps_to_fasta_eric_numba(haplos.values, var_alleles.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cff4fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#haps = haps['focal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8bda5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform xarray into haplotype array\n",
    "#haplos = allel.GenotypeArray(haps['call_genotype'].compute()).to_haplotypes()\n",
    "# extract ref and alt alleles array\n",
    "#var_alleles = haps['variant_allele'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac18b053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.69 s ± 251 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "#seq_arr = haps_to_fasta_naive(haplos.values, var_alleles.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0851071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.6 ms ± 1.35 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "#seq_arr = haps_to_fasta_numba(haplos.values, var_alleles.values)\n",
    "#seq_arr = np.vstack(seq_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dee5e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.1 ms ± 1.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "#haps_to_fasta_eric(haplos.values, var_alleles.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
